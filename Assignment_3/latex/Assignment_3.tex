\documentclass[11pt, oneside]{article}
\usepackage{geometry}
\geometry{letterpaper, margin=0.8in,top=0.75in}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{subcaption}
% \usepackage{natbib}

\title{MCSC 6020G - Numerical Analysis \\
        \Large Assignment 3}
\author{Parikshit Bajpai}
\date{}

\newtheorem*{remark}{To find}
\newtheorem*{order}{Complexity}
\renewcommand\qedsymbol{$\blacksquare$}

\begin{document}
\maketitle

\section*{Question 1}
\subsection{(a) Cholesky Decomposition}
	\begin{remark}
		The lower triangular matrix $L_N$ such that $L_N L_N^T = T_N$, where $T_N \in \mathbb{R}^{N \times N}$ is a tridiagonal SPD matrix resulting from finite difference discretisation of one-dimensional Poisson equation with homogeneous Dirichlet boundary conditions:
    \begin{equation}
      T_N = \begin{bmatrix}
      2 	& -1 		&  		&  		& \\
      -1 	& 2 		& -1 		&  		& \\
       	& \ddots 	& \ddots 	& \ddots 	& \\
       	&  		& -1	 	& 2 		& -1 \\
       	&  		&  		& -1 		& 2 \\
    \end{bmatrix}
    \end{equation}
	\end{remark}
	\begin{proof}
		Finding matrix $L_N$ is equivalent to performing a Cholesky decomposition on SPD matix $T_N$. The elements of matrix $L_N$ in terms of elements of matrix $T_N$ are, in general, given as \cite{quarteroni2017}:
    \begin{align}
      &l_{11} = \sqrt{t_{11}} \\
      &l_{ij} = \frac{\left(t_{ij} - \sum_{k=1}^{j-1} l_{ik} l_{jk} \right)}{l_{jj}} \mspace{25mu} i = 2, \dots, N;  \mspace{15mu} j = 1, \dots, i-1 \\
      &l_{ii} = \left(t_{ii} - \sum_{k=1}^{i-1} l_{ik}^2 \right)^{1/2}
    \end{align}

    This results in a banded matrix with the following elements:
    \begin{equation}\label{Generic_LN}
      l_{ij} = \begin{cases}
            \sqrt{1+\dfrac{1}{i}} & i=j\\
            \left(\dfrac{1}{i}-1\right)\sqrt{1+\dfrac{1}{i-1}} & j=i-1\\
            0 & Otherwise
          \end{cases}
      \end{equation} \qedhere
      \end{proof}

\subsection{(b) Eigenvectors and eigenvalues}
  \begin{remark}
      All eigenvectors and eigenvalues of matrix $T_N$
  \end{remark}
  \begin{proof}
    Let $v$ be the eigenvector of $T_N$. Then,
    \begin{equation}\label{eq:eigen}
      T_N v=\lambda v
    \end{equation}
    where, $\lambda$ denotes the vector of eigenvalues of $T_N$.

    The eigenvector $v$ is defined as:
    \begin{equation}
      v = \begin{bmatrix}
        \sin(\alpha) & \sin(2\alpha) & \cdots & \sin(N \alpha)
    \end{bmatrix}^T
    \end{equation}

    The $k$\textsuperscript{th} $T_n v$ is:
    \begin{equation}
      (T_N v )_k = \sum_{l=1}^{N} t_{k,l}\sin(l\alpha)
    \end{equation}
    which takes the following form for the matrix $T_N$ given above:
    \begin{align}\label{eq:Tv_exp}
      &(T_N v)_1 = 2 \sin(\alpha) -\sin(2\alpha) \\
      &(T_N v)_k = -\sin((k-1)\alpha) + 2 \sin(\alpha) -\sin((k+1)\alpha) \mspace{25mu} k = 2, \dots, N-1\\
      &(T_N v)_N = 2 \sin(N\alpha) -\sin((N-1)\alpha) \\
    \end{align}

    Using eqn.~\eqref{eq:Tv_exp} and trigonometric identity $\sin(A+B) = 2\sin(A)\cos(B)$, eqn.~\eqref{eq:eigen} can be rewritten as:
    \begin{equation}
      (T_N v)_k = \left(2 -2 cos(k\alpha) \right) v
    \end{equation}
    so that the $k$\textsuperscript{th} eigenvalue is equal to $(2 - 2\cos(k\alpha))$.

    Therefore, the eigenvector, $v$, and the eigenvalues of matrix $T_N$ can be written as:
    \begin{align}
      &v = \begin{bmatrix}
        \sin(\alpha) & \sin(2\alpha) & \cdots & \sin(N \alpha)
    \end{bmatrix}^T \\
      &\lambda = \begin{bmatrix}
        (2 - 2\cos(\alpha)) & (2 - 2\cos(2\alpha)) & \cdots & (2 - 2\cos(N\alpha))
    \end{bmatrix}^T
    \end{align} \qedhere
  \end{proof}

\section*{Question 2}
This exercise is aimed at employing the Newton-Hook method to find the root of the following function:
\begin{equation}\label{eq:rootfunc}
  f_i(x) = x_i - e^{\lambda \cos(i \sum_{k=1}^{N} x_k)}
\end{equation}
To find the optimum direction vector when the Newton step results in the solution approximation being outside the trust region, method of Lagrange multipliers is used and the resulting equation for the Lagrange multiplier is solved using a Newton-like method which uses a rational approximation to find the Lagrange multiplier. The order of convergence of Newton method is quadratic once the approximation is close enough to the true solution. However, a large amount of time might be spent in arriving close enough to the desired solution. Newton-Hook method is a trust-region based modification of Newton method to reduce the computational effort for arriving close enough to the true solution by rejecting Newton steps which would have lead to an increase in the residual and selecting an alternate residual which minimises the residual at subsequent iteration.

Eqn.~\eqref{eq:rootfunc} has a trivial solution $x = 1$ for $\lambda = 0$. This solution has been used to test the implementation of Newton-Hook algorithm and with the initial estimate as a uniform random vector with values between 1 and 1.5, Newton-Hook method converges rapidly to the trivial solution. The trust-region was halved if the residual within the trust region increased and every time a optimal direction vector was selected, the trust region was doubled. The results shown in fig.~\ref{fig:trivial} show that the optimisation process results in a rapid decrease of the residuals and convergence is achieved quadratically once the approximation gets close to the true solution.
\begin{figure}[h!]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/trivial_500.pdf}
    \caption{500 equations}
    \label{fig:trivial_1}
    \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/trivial_1000.pdf}
    \caption{1000 equations}
    \label{fig:trivial_2}
  \end{subfigure}
  \caption{Newton step size, residual, error, and trust region size vs. number of iterations for $\lambda = 0$.}
  \label{fig:trivial}
\end{figure}

The rate of convergence was tested for a range of number of equations with the same parameter values as for the trivial case. and the results are shown  in fig.~\ref{fig:conv}. As the number of equations increases, the 2 norm of the residuals increases (since it's proportional of $\sqrt{N}$), which explains the plateau as the number of equations increases. The plateau denotes the iterations which a used for arriving at a close enough approximation and beyong that the convergence is quadratic.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.75\textwidth]{figures/convergence.pdf}
  \caption{Convergence rate for the Newton-Hook method.}
  \label{fig:conv}
\end{figure}

For the other convergence tests, the parameters shown in tab.~\ref{tab:parameters} were used:
\begin{table}[h!]
\centering
\caption{Parameters used in Newton-Hook and Newton-like methods.}
\begin{tabular}{cccccc}
\toprule
{$\boldsymbol{\lambda}$} & {$\boldsymbol{\alpha}$} & {$\boldsymbol{\beta}$} & \textbf{Initial trust-region}  & \textbf{Initial estimate of solution} & \textbf{Number of equations}\\
\midrule
1  & 0.5  & 2.0 & 0.5 & Uniformly distributed random array & 25 \\
\bottomrule
\end{tabular}
\label{tab:parameters}
\end{table}

The parameter $\lambda$ affects the order of convergence of the Newton-Hook method. The number of iterations required becomes lower as $\lambda$ approaches 0. Moreover, the trust region needs to be smaller as $\lambda$ becomes larger and larger. This can lead to convergence issues as the number of equations becomes large. In the tests, Newton-Hook converged consistently for number of equations up to 25 but for larger systems, Newton-Hook failed to converge as the value of parameter $\lambda$ was changed from zero to one. The results for different values of lambda are shown in fig.~\ref{fig:lambda}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.75\textwidth]{figures/lambda.pdf}
  \caption{Convergence rate for different values of parameter $\lambda$.}
  \label{fig:lambda}
\end{figure}

The initial trust region also plays an important role in the number of iterations required for convergence. A very small initial estimate slows the progress of the code since larger Newton steps might still lead to a lower residual while a very large initial trust region can lead to increase in residuals for full Newton steps. Four different initial sizes were tested and the results have been shown in fig.~\ref{fig:trust_region}. As expected, for a very large trust region, the number of iterations spent in getting close to the true solution is large. The effect of an extremely small trust region was not observed as the smallest trust region was sufficiently large.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.75\textwidth]{figures/trust_region.pdf}
  \caption{Convergence rate for different sizes of initial trust region.}
  \label{fig:lambda}
\end{figure}

Finally, the initial guess plays a very important role in achieving convergence. The initial estimates were selected to be either equal to 1 which is the solution for trivial case or were randomnly selected from a uniform random distribution in [1,1.5]. Since the solution goes away from 1 as parameter $\lambda$ goes away from 0, selecting 1 as an initial estimate was only useful for the cases with $\lambda$ close to 0. In other cases, the uniform random distribution provided faster convergence. However, the uniform distribution is not guaranteed to provide an estimate which can ensure convergence and therefore, during testing, the method failed to converged a number of times when the random sampling was used. However, the probability of convergence with random was still more than that with 1 as the initial guess. The results are shown in fig.~\ref{fig:initest}
\begin{figure}[h!]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/initial_x1.pdf}
    \caption{1 equation}
    \label{fig:initest_1}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/initial_x2.pdf}
    \caption{10 equations}
    \label{fig:initest_2}
  \end{subfigure}
  \caption{Convergence rate for different initial estimates.}
  \label{fig:initest}
\end{figure}

In summary, selecting the optimal value of a number of parameters used in the implementation can significantly affect the rate of convergence of Newton-Hook method. The initial estimate of the solution is the most significant parameter and appropriate selection can ensure convergence. The initial trust region size and scaling parameters for the trust region can also affect the performance of the method and must be carefully studied.

\textbf{Collaboration:} This assignment was completed in collaboration with Marcos Machado. Discussions with other classmates were also quite useful in handling some of the issues.

  \bibliographystyle{ieeetr}
  \bibliography{references}
\end{document}
